{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ec696d7",
   "metadata": {},
   "source": [
    "## Model having 7 features and 2 outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "036a222c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn \n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "856a3810",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/saichaitanya/Chaitanya/CSV files/May6rd5percent_RLV_data 100 points2 .csv',header = None,names = ['h','v','s','omega','gamma','m','theta','Thrust','beta','time'])\n",
    "# df3=pd.DataFrame(df.iloc[:,0:3])\n",
    "input = df.values\n",
    "output = df.values \n",
    "X = input[:,0:7]\n",
    "y = output[:,7:9]\n",
    "# Individual Data\n",
    "h = X[:,0]\n",
    "v = X[:,1]\n",
    "s = X[:,2]\n",
    "ω = X[:,3]\n",
    "γ = X[:,4]\n",
    "m = X[:,5]\n",
    "θ = X[:,6]\n",
    "# t = X[:,3]\n",
    "T = y[:,0]\n",
    "b = y[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8a41dfde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h</th>\n",
       "      <th>v</th>\n",
       "      <th>s</th>\n",
       "      <th>omega</th>\n",
       "      <th>gamma</th>\n",
       "      <th>m</th>\n",
       "      <th>theta</th>\n",
       "      <th>Thrust</th>\n",
       "      <th>beta</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4871.8</td>\n",
       "      <td>316.67</td>\n",
       "      <td>1.168900e-16</td>\n",
       "      <td>0.342</td>\n",
       "      <td>-1.309</td>\n",
       "      <td>26230</td>\n",
       "      <td>-1.309</td>\n",
       "      <td>483430.0</td>\n",
       "      <td>0.056463</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        h       v             s  omega  gamma      m  theta    Thrust  \\\n",
       "0  4871.8  316.67  1.168900e-16  0.342 -1.309  26230 -1.309  483430.0   \n",
       "\n",
       "       beta  time  \n",
       "0  0.056463   0.0  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f342f0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "X_norm = preprocessing.minmax_scale(X)\n",
    "y_norm = preprocessing.minmax_scale(y)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_norm, y_norm, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "09a48672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(n):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n,input_shape=(7,),kernel_initializer='uniform'))\n",
    "    model.add(Dense(n,kernel_initializer='uniform',activation = 'relu'))\n",
    "    model.add(Dense(n,kernel_initializer='uniform',activation = 'tanh')) # since tanh has more nonlinearity we add it here, it also gives -ve values so , some layers which are not necessary will lead to 0 in next layer\n",
    "    model.add(Dense(n,kernel_initializer='uniform',activation = 'relu'))\n",
    "    model.add(Dense(2,kernel_initializer='uniform',activation = 'sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "789d6088",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Number of Hidden units used is:  256\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.05.\n",
      "Epoch 1/100\n",
      "5523/5523 [==============================] - 23s 4ms/step - loss: 0.2176 - accuracy: 0.7472 - lr: 0.0500\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.05.\n",
      "Epoch 2/100\n",
      "5523/5523 [==============================] - 22s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 0.0500\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.05.\n",
      "Epoch 3/100\n",
      "5523/5523 [==============================] - 23s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 0.0500\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.05.\n",
      "Epoch 4/100\n",
      "5523/5523 [==============================] - 23s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 0.0500\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.05.\n",
      "Epoch 5/100\n",
      "5505/5523 [============================>.] - ETA: 0s - loss: 0.2176 - accuracy: 0.7473\n",
      "Epoch 5: saving model to /home/saichaitanya/Chaitanya/Trained models /7 input and 2 output features/256/cp.ckpt\n",
      "5523/5523 [==============================] - 23s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 0.0500\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.0318814.\n",
      "Epoch 6/100\n",
      "5523/5523 [==============================] - 23s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 0.0319\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.0318814.\n",
      "Epoch 7/100\n",
      "5523/5523 [==============================] - 23s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 0.0319\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.0318814.\n",
      "Epoch 8/100\n",
      "5523/5523 [==============================] - 23s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 0.0319\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.0318814.\n",
      "Epoch 9/100\n",
      "5523/5523 [==============================] - 23s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 0.0319\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.0318814.\n",
      "Epoch 10/100\n",
      "5505/5523 [============================>.] - ETA: 0s - loss: 0.2177 - accuracy: 0.7472\n",
      "Epoch 10: saving model to /home/saichaitanya/Chaitanya/Trained models /7 input and 2 output features/256/cp.ckpt\n",
      "5523/5523 [==============================] - 23s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 0.0319\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 0.0203285.\n",
      "Epoch 11/100\n",
      "5523/5523 [==============================] - 23s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 0.0203\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 0.0203285.\n",
      "Epoch 12/100\n",
      "5523/5523 [==============================] - 23s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 0.0203\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 0.0203285.\n",
      "Epoch 13/100\n",
      "5523/5523 [==============================] - 23s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 0.0203\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 0.0203285.\n",
      "Epoch 14/100\n",
      "5523/5523 [==============================] - 23s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 0.0203\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 0.0203285.\n",
      "Epoch 15/100\n",
      "5503/5523 [============================>.] - ETA: 0s - loss: 0.2176 - accuracy: 0.7473\n",
      "Epoch 15: saving model to /home/saichaitanya/Chaitanya/Trained models /7 input and 2 output features/256/cp.ckpt\n",
      "5523/5523 [==============================] - 23s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 0.0203\n",
      "\n",
      "Epoch 16: LearningRateScheduler setting learning rate to 0.012962.\n",
      "Epoch 16/100\n",
      "5523/5523 [==============================] - 23s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 0.0130\n",
      "\n",
      "Epoch 17: LearningRateScheduler setting learning rate to 0.012962.\n",
      "Epoch 17/100\n",
      "5523/5523 [==============================] - 23s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 0.0130\n",
      "\n",
      "Epoch 18: LearningRateScheduler setting learning rate to 0.012962.\n",
      "Epoch 18/100\n",
      "5523/5523 [==============================] - 22s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 0.0130\n",
      "\n",
      "Epoch 19: LearningRateScheduler setting learning rate to 0.012962.\n",
      "Epoch 19/100\n",
      "5523/5523 [==============================] - 23s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 0.0130\n",
      "\n",
      "Epoch 20: LearningRateScheduler setting learning rate to 0.012962.\n",
      "Epoch 20/100\n",
      "5500/5523 [============================>.] - ETA: 0s - loss: 0.2176 - accuracy: 0.7472\n",
      "Epoch 20: saving model to /home/saichaitanya/Chaitanya/Trained models /7 input and 2 output features/256/cp.ckpt\n",
      "5523/5523 [==============================] - 23s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 0.0130\n",
      "\n",
      "Epoch 21: LearningRateScheduler setting learning rate to 0.0082649.\n",
      "Epoch 21/100\n",
      "5523/5523 [==============================] - 23s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 0.0083\n",
      "\n",
      "Epoch 22: LearningRateScheduler setting learning rate to 0.0082649.\n",
      "Epoch 22/100\n",
      "5523/5523 [==============================] - 24s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 0.0083\n",
      "\n",
      "Epoch 23: LearningRateScheduler setting learning rate to 0.0082649.\n",
      "Epoch 23/100\n",
      "5523/5523 [==============================] - 23s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 0.0083\n",
      "\n",
      "Epoch 24: LearningRateScheduler setting learning rate to 0.0082649.\n",
      "Epoch 24/100\n",
      "5523/5523 [==============================] - 23s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 0.0083\n",
      "\n",
      "Epoch 25: LearningRateScheduler setting learning rate to 0.0082649.\n",
      "Epoch 25/100\n",
      "5493/5523 [============================>.] - ETA: 0s - loss: 0.2177 - accuracy: 0.7473\n",
      "Epoch 25: saving model to /home/saichaitanya/Chaitanya/Trained models /7 input and 2 output features/256/cp.ckpt\n",
      "5523/5523 [==============================] - 23s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 0.0083\n",
      "\n",
      "Epoch 26: LearningRateScheduler setting learning rate to 0.0052699.\n",
      "Epoch 26/100\n",
      "5523/5523 [==============================] - 24s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 0.0053\n",
      "\n",
      "Epoch 27: LearningRateScheduler setting learning rate to 0.0052699.\n",
      "Epoch 27/100\n",
      "5523/5523 [==============================] - 23s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 0.0053\n",
      "\n",
      "Epoch 28: LearningRateScheduler setting learning rate to 0.0052699.\n",
      "Epoch 28/100\n",
      "5523/5523 [==============================] - 24s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 0.0053\n",
      "\n",
      "Epoch 29: LearningRateScheduler setting learning rate to 0.0052699.\n",
      "Epoch 29/100\n",
      "5523/5523 [==============================] - 23s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 0.0053\n",
      "\n",
      "Epoch 30: LearningRateScheduler setting learning rate to 0.0052699.\n",
      "Epoch 30/100\n",
      "5488/5523 [============================>.] - ETA: 0s - loss: 0.2176 - accuracy: 0.7472\n",
      "Epoch 30: saving model to /home/saichaitanya/Chaitanya/Trained models /7 input and 2 output features/256/cp.ckpt\n",
      "5523/5523 [==============================] - 23s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 0.0053\n",
      "\n",
      "Epoch 31: LearningRateScheduler setting learning rate to 0.0033602.\n",
      "Epoch 31/100\n",
      "5523/5523 [==============================] - 23s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 0.0034\n",
      "\n",
      "Epoch 32: LearningRateScheduler setting learning rate to 0.0033602.\n",
      "Epoch 32/100\n",
      "5523/5523 [==============================] - 24s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 0.0034\n",
      "\n",
      "Epoch 33: LearningRateScheduler setting learning rate to 0.0033602.\n",
      "Epoch 33/100\n",
      "5523/5523 [==============================] - 24s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 0.0034\n",
      "\n",
      "Epoch 34: LearningRateScheduler setting learning rate to 0.0033602.\n",
      "Epoch 34/100\n",
      "5523/5523 [==============================] - 24s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 0.0034\n",
      "\n",
      "Epoch 35: LearningRateScheduler setting learning rate to 0.0033602.\n",
      "Epoch 35/100\n",
      "5480/5523 [============================>.] - ETA: 0s - loss: 0.2177 - accuracy: 0.7472\n",
      "Epoch 35: saving model to /home/saichaitanya/Chaitanya/Trained models /7 input and 2 output features/256/cp.ckpt\n",
      "5523/5523 [==============================] - 23s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 0.0034\n",
      "\n",
      "Epoch 36: LearningRateScheduler setting learning rate to 0.0021426.\n",
      "Epoch 36/100\n",
      "5523/5523 [==============================] - 23s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 0.0021\n",
      "\n",
      "Epoch 37: LearningRateScheduler setting learning rate to 0.0021426.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/100\n",
      "5523/5523 [==============================] - 22s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 0.0021\n",
      "\n",
      "Epoch 38: LearningRateScheduler setting learning rate to 0.0021426.\n",
      "Epoch 38/100\n",
      "5523/5523 [==============================] - 22s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 0.0021\n",
      "\n",
      "Epoch 39: LearningRateScheduler setting learning rate to 0.0021426.\n",
      "Epoch 39/100\n",
      "5523/5523 [==============================] - 22s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 0.0021\n",
      "\n",
      "Epoch 40: LearningRateScheduler setting learning rate to 0.0021426.\n",
      "Epoch 40/100\n",
      "5479/5523 [============================>.] - ETA: 0s - loss: 0.2176 - accuracy: 0.7474\n",
      "Epoch 40: saving model to /home/saichaitanya/Chaitanya/Trained models /7 input and 2 output features/256/cp.ckpt\n",
      "5523/5523 [==============================] - 22s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 0.0021\n",
      "\n",
      "Epoch 41: LearningRateScheduler setting learning rate to 0.0013662.\n",
      "Epoch 41/100\n",
      "5523/5523 [==============================] - 22s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 0.0014\n",
      "\n",
      "Epoch 42: LearningRateScheduler setting learning rate to 0.0013662.\n",
      "Epoch 42/100\n",
      "5523/5523 [==============================] - 22s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 0.0014\n",
      "\n",
      "Epoch 43: LearningRateScheduler setting learning rate to 0.0013662.\n",
      "Epoch 43/100\n",
      "5523/5523 [==============================] - 22s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 0.0014\n",
      "\n",
      "Epoch 44: LearningRateScheduler setting learning rate to 0.0013662.\n",
      "Epoch 44/100\n",
      "5523/5523 [==============================] - 22s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 0.0014\n",
      "\n",
      "Epoch 45: LearningRateScheduler setting learning rate to 0.0013662.\n",
      "Epoch 45/100\n",
      "5466/5523 [============================>.] - ETA: 0s - loss: 0.2176 - accuracy: 0.7474\n",
      "Epoch 45: saving model to /home/saichaitanya/Chaitanya/Trained models /7 input and 2 output features/256/cp.ckpt\n",
      "5523/5523 [==============================] - 22s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 0.0014\n",
      "\n",
      "Epoch 46: LearningRateScheduler setting learning rate to 0.0008711.\n",
      "Epoch 46/100\n",
      "5523/5523 [==============================] - 22s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 8.7110e-04\n",
      "\n",
      "Epoch 47: LearningRateScheduler setting learning rate to 0.0008711.\n",
      "Epoch 47/100\n",
      "5523/5523 [==============================] - 22s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 8.7110e-04\n",
      "\n",
      "Epoch 48: LearningRateScheduler setting learning rate to 0.0008711.\n",
      "Epoch 48/100\n",
      "5523/5523 [==============================] - 22s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 8.7110e-04\n",
      "\n",
      "Epoch 49: LearningRateScheduler setting learning rate to 0.0008711.\n",
      "Epoch 49/100\n",
      "5523/5523 [==============================] - 22s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 8.7110e-04\n",
      "\n",
      "Epoch 50: LearningRateScheduler setting learning rate to 0.0008711.\n",
      "Epoch 50/100\n",
      "5464/5523 [============================>.] - ETA: 0s - loss: 0.2176 - accuracy: 0.7473\n",
      "Epoch 50: saving model to /home/saichaitanya/Chaitanya/Trained models /7 input and 2 output features/256/cp.ckpt\n",
      "5523/5523 [==============================] - 22s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 8.7110e-04\n",
      "\n",
      "Epoch 51: LearningRateScheduler setting learning rate to 0.0005554.\n",
      "Epoch 51/100\n",
      "5523/5523 [==============================] - 22s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 5.5540e-04\n",
      "\n",
      "Epoch 52: LearningRateScheduler setting learning rate to 0.0005554.\n",
      "Epoch 52/100\n",
      "5523/5523 [==============================] - 22s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 5.5540e-04\n",
      "\n",
      "Epoch 53: LearningRateScheduler setting learning rate to 0.0005554.\n",
      "Epoch 53/100\n",
      "5523/5523 [==============================] - 22s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 5.5540e-04\n",
      "\n",
      "Epoch 54: LearningRateScheduler setting learning rate to 0.0005554.\n",
      "Epoch 54/100\n",
      "5523/5523 [==============================] - 22s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 5.5540e-04\n",
      "\n",
      "Epoch 55: LearningRateScheduler setting learning rate to 0.0005554.\n",
      "Epoch 55/100\n",
      "5466/5523 [============================>.] - ETA: 0s - loss: 0.2177 - accuracy: 0.7473\n",
      "Epoch 55: saving model to /home/saichaitanya/Chaitanya/Trained models /7 input and 2 output features/256/cp.ckpt\n",
      "5523/5523 [==============================] - 22s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 5.5540e-04\n",
      "\n",
      "Epoch 56: LearningRateScheduler setting learning rate to 0.0003541.\n",
      "Epoch 56/100\n",
      "5523/5523 [==============================] - 22s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 3.5410e-04\n",
      "\n",
      "Epoch 57: LearningRateScheduler setting learning rate to 0.0003541.\n",
      "Epoch 57/100\n",
      "5523/5523 [==============================] - 22s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 3.5410e-04\n",
      "\n",
      "Epoch 58: LearningRateScheduler setting learning rate to 0.0003541.\n",
      "Epoch 58/100\n",
      "5523/5523 [==============================] - 22s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 3.5410e-04\n",
      "\n",
      "Epoch 59: LearningRateScheduler setting learning rate to 0.0003541.\n",
      "Epoch 59/100\n",
      "5523/5523 [==============================] - 22s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 3.5410e-04\n",
      "\n",
      "Epoch 60: LearningRateScheduler setting learning rate to 0.0003541.\n",
      "Epoch 60/100\n",
      "5451/5523 [============================>.] - ETA: 0s - loss: 0.2176 - accuracy: 0.7475\n",
      "Epoch 60: saving model to /home/saichaitanya/Chaitanya/Trained models /7 input and 2 output features/256/cp.ckpt\n",
      "5523/5523 [==============================] - 22s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 3.5410e-04\n",
      "\n",
      "Epoch 61: LearningRateScheduler setting learning rate to 0.0002258.\n",
      "Epoch 61/100\n",
      "5523/5523 [==============================] - 22s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 2.2580e-04\n",
      "\n",
      "Epoch 62: LearningRateScheduler setting learning rate to 0.0002258.\n",
      "Epoch 62/100\n",
      "5523/5523 [==============================] - 23s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 2.2580e-04\n",
      "\n",
      "Epoch 63: LearningRateScheduler setting learning rate to 0.0002258.\n",
      "Epoch 63/100\n",
      "5523/5523 [==============================] - 21s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 2.2580e-04\n",
      "\n",
      "Epoch 64: LearningRateScheduler setting learning rate to 0.0002258.\n",
      "Epoch 64/100\n",
      "5523/5523 [==============================] - 21s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 2.2580e-04\n",
      "\n",
      "Epoch 65: LearningRateScheduler setting learning rate to 0.0002258.\n",
      "Epoch 65/100\n",
      "5456/5523 [============================>.] - ETA: 0s - loss: 0.2176 - accuracy: 0.7473\n",
      "Epoch 65: saving model to /home/saichaitanya/Chaitanya/Trained models /7 input and 2 output features/256/cp.ckpt\n",
      "5523/5523 [==============================] - 22s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 2.2580e-04\n",
      "\n",
      "Epoch 66: LearningRateScheduler setting learning rate to 0.000144.\n",
      "Epoch 66/100\n",
      "5523/5523 [==============================] - 22s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 1.4400e-04\n",
      "\n",
      "Epoch 67: LearningRateScheduler setting learning rate to 0.000144.\n",
      "Epoch 67/100\n",
      "5523/5523 [==============================] - 22s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 1.4400e-04\n",
      "\n",
      "Epoch 68: LearningRateScheduler setting learning rate to 0.000144.\n",
      "Epoch 68/100\n",
      "5523/5523 [==============================] - 22s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 1.4400e-04\n",
      "\n",
      "Epoch 69: LearningRateScheduler setting learning rate to 0.000144.\n",
      "Epoch 69/100\n",
      "5523/5523 [==============================] - 22s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 1.4400e-04\n",
      "\n",
      "Epoch 70: LearningRateScheduler setting learning rate to 0.000144.\n",
      "Epoch 70/100\n",
      "5443/5523 [============================>.] - ETA: 0s - loss: 0.2176 - accuracy: 0.7474\n",
      "Epoch 70: saving model to /home/saichaitanya/Chaitanya/Trained models /7 input and 2 output features/256/cp.ckpt\n",
      "5523/5523 [==============================] - 22s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 1.4400e-04\n",
      "\n",
      "Epoch 71: LearningRateScheduler setting learning rate to 9.18e-05.\n",
      "Epoch 71/100\n",
      "5523/5523 [==============================] - 22s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 9.1800e-05\n",
      "\n",
      "Epoch 72: LearningRateScheduler setting learning rate to 9.18e-05.\n",
      "Epoch 72/100\n",
      "5523/5523 [==============================] - 22s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 9.1800e-05\n",
      "\n",
      "Epoch 73: LearningRateScheduler setting learning rate to 9.18e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/100\n",
      "5523/5523 [==============================] - 23s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 9.1800e-05\n",
      "\n",
      "Epoch 74: LearningRateScheduler setting learning rate to 9.18e-05.\n",
      "Epoch 74/100\n",
      "5523/5523 [==============================] - 22s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 9.1800e-05\n",
      "\n",
      "Epoch 75: LearningRateScheduler setting learning rate to 9.18e-05.\n",
      "Epoch 75/100\n",
      "5439/5523 [============================>.] - ETA: 0s - loss: 0.2177 - accuracy: 0.7473\n",
      "Epoch 75: saving model to /home/saichaitanya/Chaitanya/Trained models /7 input and 2 output features/256/cp.ckpt\n",
      "5523/5523 [==============================] - 22s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 9.1800e-05\n",
      "\n",
      "Epoch 76: LearningRateScheduler setting learning rate to 5.85e-05.\n",
      "Epoch 76/100\n",
      "5523/5523 [==============================] - 22s 4ms/step - loss: 0.2176 - accuracy: 0.7473 - lr: 5.8500e-05\n",
      "\n",
      "Epoch 77: LearningRateScheduler setting learning rate to 5.85e-05.\n",
      "Epoch 77/100\n",
      "4733/5523 [========================>.....] - ETA: 3s - loss: 0.2177 - accuracy: 0.7471"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [56]\u001b[0m, in \u001b[0;36m<cell line: 58>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer \u001b[38;5;241m=\u001b[39m opt, loss \u001b[38;5;241m=\u001b[39m loss, metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# model.load_weights(checkpoint_path)\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatchsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcp_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m model\u001b[38;5;241m.\u001b[39mevaluate(X_test,y_test)\n",
      "File \u001b[0;32m~/anaconda3/envs/saiaero/lib/python3.10/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/saiaero/lib/python3.10/site-packages/keras/engine/training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1379\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1380\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1381\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1382\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1383\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1384\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1385\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1386\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/anaconda3/envs/saiaero/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/saiaero/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/envs/saiaero/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/envs/saiaero/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2953\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2954\u001b[0m   (graph_function,\n\u001b[1;32m   2955\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/saiaero/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1849\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1852\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1853\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1855\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m     args,\n\u001b[1;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1858\u001b[0m     executing_eagerly)\n\u001b[1;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/envs/saiaero/lib/python3.10/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/anaconda3/envs/saiaero/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# for i in range(4,11,1):\n",
    "i = 8\n",
    "model = create_model(2**i)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate = 0.05)\n",
    "loss = tf.keras.losses.mean_squared_error\n",
    "def lr_sch(epoch,lr):\n",
    "    if (epoch>0) & (epoch<200):\n",
    "      if epoch%50==0: # for every 100 epochs the learning rate varies as metioned. \n",
    "        return lr*np.exp(-0.45)\n",
    "      else:\n",
    "        return lr\n",
    "    elif(epoch>200):\n",
    "        if epoch%40==0:\n",
    "            return lr*np.exp(-0.225);\n",
    "        else:\n",
    "            return lr\n",
    "    else:\n",
    "        return lr\n",
    "\n",
    "def lr_sch2(epoch,lr):\n",
    "    if (epoch>0) & (epoch<200):\n",
    "      if epoch%5==0: # for every 100 epochs the learning rate varies as metioned. \n",
    "        return round(lr*np.exp(-0.45),7)\n",
    "      else:\n",
    "        return round(lr,7)\n",
    "    elif(epoch>200):\n",
    "        if epoch%20==0:\n",
    "            return round(lr*np.exp(-0.125),7)\n",
    "        else:\n",
    "            return round(lr,7)\n",
    "    else:\n",
    "        return round(lr,7)\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_sch2,verbose = 1)\n",
    "\n",
    "print(\"The Number of Hidden units used is: \",2**i)\n",
    "# lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_sch,verbose = 0)\n",
    "\n",
    "\n",
    "checkpoint_path =  f\"/home/saichaitanya/Chaitanya/Trained models /7 input and 2 output features/{2**i}/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# BATCH_SIZE = 128\n",
    "batchsize = 32\n",
    "STEPS_PER_EPOCH = X_train.shape[0] / batchsize\n",
    "save_period = 5\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                              save_weights_only=True,\n",
    "                                              verbose=1,save_freq=int(save_period*STEPS_PER_EPOCH))\n",
    "\n",
    "# This may generate warnings related to saving the state of the optimizer.\n",
    "# These warnings (and similar warnings throughout this notebook)\n",
    "# are in place to discourage outdated usage, and can be ignored.\n",
    "\n",
    "model.compile(optimizer = opt, loss = loss, metrics = 'accuracy')\n",
    "# model.load_weights(checkpoint_path)\n",
    "model.fit(X_train,y_train,epochs = 100,batch_size = batchsize,shuffle = True,use_multiprocessing = True,callbacks=[lr_scheduler,cp_callback])\n",
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd1df61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
